{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from src.datasets.heart_dataset import HeartDataset\n",
    "from src.datasets.german_dataset import GermanDataset\n",
    "from src.datasets.adult_sampled_dataset import AdultSampledDataset\n",
    "from src.experiments import init_dataset\n",
    "random_state = 42\n",
    "\n",
    "data_path = '../data'\n",
    "german = GermanDataset(f'{data_path}/german_credit/german.data', binary=True, group_type='',\n",
    "                             random_state=random_state)\n",
    "# adult = init_dataset('adult', 42)\n",
    "german = init_dataset('bank', 42)\n",
    "heart = HeartDataset(f'{data_path}/heart_disease/processed.cleveland.data', binary=True, group_type='',\n",
    "                            random_state=random_state)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_features_groups_target(df, dataset):\n",
    "    features = df[[c for c in df.columns if c not in [*dataset.sensitive, dataset.target]]]\n",
    "    sensitive = df[dataset.sensitive]\n",
    "    target = df[[dataset.target]]\n",
    "    return features, sensitive, target"
   ],
   "id": "98b48f58958d6709",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "german_df = german.data\n",
    "german_np = MinMaxScaler().fit_transform(german_df)\n",
    "german_df = pd.DataFrame(german_np, columns=german_df.columns)\n",
    "# adult_df = adult.data#adult.perform_encoding('cont_ord_cat', adult.data, adult.data)\n",
    "# bank_df = bank.data#bank.perform_encoding('cont_ord_cat', bank.data, bank.data)\n",
    "\n",
    "german_features, german_sensitive, german_target = get_features_groups_target(german_df, german)\n",
    "# adult_features, adult_sensitive, adult_target = get_features_groups_target(adult_df, adult)\n",
    "# bank_features, bank_sensitive, bank_target = get_features_groups_target(bank_df, bank)\n",
    "# dataset = AdultSampledDataset(f'{data_path}/adult_census/sampled_sex/natural.csv', binary=True, group_type='',\n",
    "#                             random_state=42)\n",
    "# german_df = dataset.data\n",
    "# german_np = MinMaxScaler().fit_transform(german_df)\n",
    "# german_df = pd.DataFrame(german_np, columns=german_df.columns)\n",
    "# adult_df = adult.data#adult.perform_encoding('cont_ord_cat', adult.data, adult.data)\n",
    "# bank_df = bank.data#bank.perform_encoding('cont_ord_cat', bank.data, bank.data)\n",
    "\n",
    "german_features, german_sensitive, german_target = get_features_groups_target(german_df, german)"
   ],
   "id": "aad10774193767d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def pca_visualization(features, sensitive, target):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(features)\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    pca_features = pca.transform(features)\n",
    "    sensitive_groups = sensitive.astype('str').agg('_'.join, axis=1)\n",
    "    group_class = [f'{t}_{s}' for t, s in zip(target.to_list(), sensitive_groups.to_list())]    \n",
    "    PC1 = pca_features[:,0]\n",
    "    PC2 = pca_features[:,1]\n",
    "    df = pd.DataFrame({'PC1': PC1, 'PC2': PC2, 'target': target, 'sensitive_groups': sensitive_groups, 'group_class': group_class})\n",
    "    axs[0] = sns.scatterplot(df, x='PC1', y='PC2', hue='group_class', ax=axs[0])\n",
    "    sns.scatterplot(df, x='PC2', y='PC1', hue='target', ax=axs[1])\n",
    "    sns.scatterplot(df, x='PC2', y='PC1', hue='sensitive_groups', ax=axs[2])\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "5b4665a7db3b352d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pca_visualization(german_features, german_sensitive, german_target[german.target])",
   "id": "aec51661d0fbd3a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import SparsePCA, KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "def jitter(values,j):\n",
    "    return values + np.random.normal(j,0.1,values.shape)\n",
    "\n",
    "def tsne(fair, dataset):\n",
    "    features_fair, sensitive_fair, target_fair = get_features_groups_target(fair, dataset)\n",
    "    target_fair = target_fair[dataset.target]\n",
    "    #fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    # pca = TSNE(n_components=2, learning_rate='auto',\n",
    "    #             init='pca', perplexity=50, random_state=42,n_jobs=-1) #PCA(n_components=2, random_state=42)\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    #pca.fit(features_fair)\n",
    "    \n",
    "    #print(pca.explained_variance_ratio_)\n",
    "    \n",
    "    pca_features = pca.fit_transform(features_fair)\n",
    "    return pca_features\n",
    "\n",
    "\n",
    "def pca_visualization2(data, fair, dataset, fig, axs, e, alg_name, fair_all, pca_features_all):\n",
    "    features, sensitive, target = get_features_groups_target(data, dataset)\n",
    "    target = target[dataset.target]\n",
    "    #fair = pd.concat(fair_all)\n",
    "    lens_fair = [len(f) for f in fair_all]\n",
    "    lens_fair = [0, *lens_fair]\n",
    "    features_fair, sensitive_fair, target_fair = get_features_groups_target(fair, dataset)\n",
    "    target_fair = target_fair[dataset.target]\n",
    "    #fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    # pca = TSNE(n_components=2, learning_rate='auto',\n",
    "    #               init='random', perplexity=3, random_state=42) #PCA(n_components=2, random_state=42)\n",
    "    # pca = PCA(n_components=2, random_state=42)\n",
    "    # pca.fit(features_fair)\n",
    "    \n",
    "    #print(pca.explained_variance_ratio_)\n",
    "    \n",
    "    # pca_features = pca.fit_transform(features_fair)\n",
    "    pca_features = pca_features_all[np.sum(lens_fair[:e+1]):np.sum(lens_fair[:e+2]), :]\n",
    "    assert len(pca_features) == len(features), (lens_fair, e, len(pca_features), len(features))\n",
    "    sensitive_groups = sensitive.astype('int').astype('str').agg('_'.join, axis=1)\n",
    "    group_class = [f'{s}_{int(t)}' for t, s in zip(target.to_list(), sensitive_groups.to_list())]    \n",
    "    PC1 = pca_features[:,0]\n",
    "    PC2 = pca_features[:,1]\n",
    "    df = pd.DataFrame({'PC1': PC1, 'PC2': PC2, 'target': target, 'sensitive_groups': sensitive_groups, 'group_class': group_class})\n",
    "    order_groups = np.sort(np.unique(sensitive_groups).flatten())\n",
    "    order_class = [int(dataset.minority), int(dataset.majority)]#np.sort(np.unique(target).flatten())\n",
    "    order_group_class = [*['_'.join([str(int(pi)) for pi in p.values()]) for p in dataset.privileged_groups], *['_'.join([str(int(pi)) for pi in p.values()]) for p in dataset.unprivileged_groups]]\n",
    "    order_group_class = [f'{g}_{int(c)}' for c in order_class for g in order_group_class]\n",
    "    #order_group_class = np.sort(np.unique(group_class).flatten())\n",
    "    markers = {dataset.minority: 'o', dataset.majority: '^'}\n",
    "    \n",
    "    axs[e//3, e%3] = sns.scatterplot(df, x='PC1', y='PC2', hue='group_class', hue_order=order_group_class, ax=axs[e//3, e%3], style='target', style_order=order_class, legend=False, markers=markers).set_title(f'{alg_name}')\n",
    "    \n",
    "    # sns.scatterplot(df, x='PC1', y='PC2', hue='target', hue_order=order_class, ax=axs[e, 1]).set_title(f'{alg_name}')\n",
    "    # sns.scatterplot(df, x='PC1', y='PC2', hue='sensitive_groups', hue_order=order_groups, ax=axs[e, 2]).set_title(f'{alg_name}')\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ],
   "id": "19b97d0d3ea3d728",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def explain_data(data, fair, dataset, fig, axs, e, alg_name, pca_features_all):\n",
    "    data_np = MinMaxScaler().fit_transform(data)\n",
    "    data_df = pd.DataFrame(data_np, columns=data.columns)\n",
    "    fair_all = pd.concat(fair)\n",
    "    fair_np = MinMaxScaler().fit_transform(fair_all)\n",
    "    fair_df = pd.DataFrame(fair_np, columns=fair_all.columns)\n",
    "    # data_df = dataset.perform_encoding('cont_ord_cat', fair_all, data)\n",
    "    # fair_df = dataset.perform_encoding('cont_ord_cat', fair_all, fair_all)\n",
    "    return pca_visualization2(data_df, fair_df, dataset, fig, axs, e, alg_name, fair, pca_features_all)"
   ],
   "id": "e3694838ab28726e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_mistakes_groups(test_data, y_test, y_pred, dataset):\n",
    "    features, sensitive, target = get_features_groups_target(test_data, dataset)\n",
    "    target = target[dataset.target]\n",
    "    sensitive_groups = sensitive.astype('str').agg('_'.join, axis=1)\n",
    "    group_class = [f'{s}_{t}' for t, s in zip(target.to_list(), sensitive_groups.to_list())]\n",
    "    for g in np.unique(group_class):\n",
    "        print(g)\n",
    "        y_test_g = np.array(y_test)[np.array(group_class) == g]\n",
    "        y_pred_g = y_pred[np.array(group_class) == g]\n",
    "        print(y_test_g, y_pred_g)\n",
    "        print(np.sum(y_test_g == y_pred_g), len(y_test_g))\n",
    "        print('-----')"
   ],
   "id": "c12ef5648120a76d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.datasets.bank_dataset import BankDataset\n",
    "from src.datasets.adult_sampled_dataset import AdultSampledDataset\n",
    "import os\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "dataset_name = 'bank'\n",
    "data_path = '../data'\n",
    "algorithms = ['Fair-RBU', 'Fair-RBH', 'HFOS', 'FOS', 'FAWOS']\n",
    "save_path = '../figures'\n",
    "multi_bin = 'bin'\n",
    "results_path = '../results'\n",
    "\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    all_data = []\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    dataset = init_dataset(dataset_name, random_state=42, data_path=data_path)\n",
    "    for e, path in enumerate([f'{results_path}/fair_rbu_{dataset_name}_decision_tree/mean/', f'{results_path}/fair_rbh_{dataset_name}_decision_tree/2024-07-14/', f'{results_path}/hfos_{dataset_name}_decision_tree/2024-06-29/', f'{results_path}/fos_{dataset_name}_decision_tree/2024-06-29/', f'{results_path}/fawos_{dataset_name}_decision_tree/2024-06-29/']):\n",
    "        normal_data = os.path.join(path, f'train_{i}.csv')\n",
    "        fair_data = os.path.join(path, f'fair_{i}.csv')\n",
    "        if e == 0:\n",
    "            normal_data = pd.read_csv(normal_data)\n",
    "            normal_data = normal_data.iloc[: , 1:]\n",
    "            all_data.append(normal_data)\n",
    "        fair_data = pd.read_csv(fair_data)\n",
    "        fair_data = fair_data.iloc[: , 1:]\n",
    "        \n",
    "        \n",
    "        all_data.append(fair_data)\n",
    "    fair_all = pd.concat(all_data)\n",
    "    fair_df = dataset.perform_encoding('cont_ord_cat', fair_all, fair_all)\n",
    "    pca_features_all = tsne(fair_df, dataset)\n",
    "    print('done')\n",
    "    for e, path in enumerate([f'{results_path}/fair_rbu_{dataset_name}_decision_tree/mean/', f'{results_path}/fair_rbh_{dataset_name}_decision_tree/2024-07-14/', f'{results_path}/hfos_{dataset_name}_decision_tree/2024-06-29/', f'{results_path}/fos_{dataset_name}_decision_tree/2024-06-29/', f'{results_path}/fawos_{dataset_name}_decision_tree/2024-06-29/']):\n",
    "        normal_data = os.path.join(path, f'train_{i}.csv')\n",
    "        fair_data = os.path.join(path, f'fair_{i}.csv')\n",
    "        results = os.path.join(path, f'fairness_{i}.csv')\n",
    "        performance_res = os.path.join(path, f'performance_{i}.csv')\n",
    "        test_data = os.path.join(path, f'test_{i}.csv')\n",
    "        preds_train = os.path.join(path, f'train_preds_{i}.npy')\n",
    "        preds_fair = os.path.join(path, f'fair_preds_{i}.npy')\n",
    "        \n",
    "        normal_data = pd.read_csv(normal_data)\n",
    "        normal_data = normal_data.iloc[: , 1:]\n",
    "        fair_data = pd.read_csv(fair_data)\n",
    "        fair_data = fair_data.iloc[: , 1:]\n",
    "        results = pd.read_csv(results)\n",
    "        performance = pd.read_csv(performance_res)\n",
    "        \n",
    "        if e == 0:\n",
    "            fig = explain_data(normal_data, all_data, dataset, fig, axs, e, '-', pca_features_all)\n",
    "        \n",
    "        fig = explain_data(fair_data, all_data, dataset, fig, axs, e + 1, algorithms[e], pca_features_all)\n",
    "    # for ax in axs:\n",
    "    #     ax.get_legend().set_visible(False)\n",
    "    plt.savefig(f'{save_path}/{dataset_name}_{multi_bin}.pdf')\n",
    "    plt.show()\n",
    "    print('========================================================================')"
   ],
   "id": "5e2badd7a92e4908",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.validation_fhf_multi import init_dataset_multi\n",
    "from src.datasets.bank_dataset import BankDataset\n",
    "from src.datasets.adult_sampled_dataset import AdultSampledDataset\n",
    "import os\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "dataset_name = 'bank'\n",
    "data_path = '../data'\n",
    "algorithms = ['Fair-RBU', 'Fair-RBH', 'HFOS', 'FAWOS']\n",
    "save_path = '../figures'\n",
    "multi_bin = 'multi'\n",
    "results_path = '../results_multi'\n",
    "\n",
    "for i in range(0, 1):\n",
    "    print(i)\n",
    "    all_data = []\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    dataset = init_dataset_multi(dataset_name, random_state=42, data_path=data_path)\n",
    "    #dataset = init_dataset_(dataset_name, random_state=42, data_path=data_path)\n",
    "    for e, path in enumerate([f'{results_path}/fair_rbu_{dataset_name}_decision_tree/2024-07-14/', f'{results_path}/fair_rbh_{dataset_name}_decision_tree/2024-07-14/', f'{results_path}/hfos_{dataset_name}_decision_tree/2024-06-29/', f'{results_path}/fawos_{dataset_name}_decision_tree/2024-06-29/']):\n",
    "        normal_data = os.path.join(path, f'train_{i}.csv')\n",
    "        fair_data = os.path.join(path, f'fair_{i}.csv')\n",
    "        if e == 0:\n",
    "            normal_data = pd.read_csv(normal_data)\n",
    "            normal_data = normal_data.iloc[: , 1:]\n",
    "            all_data.append(normal_data)\n",
    "        fair_data = pd.read_csv(fair_data)\n",
    "        fair_data = fair_data.iloc[: , 1:]\n",
    "        features, sensitive, target = get_features_groups_target(fair_data, dataset)\n",
    "        target = target[dataset.target]\n",
    "        sensitive_groups = sensitive.astype('str').agg('_'.join, axis=1)\n",
    "        group_class = [f'{s}_{t}' for t, s in zip(target.to_list(), sensitive_groups.to_list())]\n",
    "        ids = [i for i, gg in enumerate(group_class) if gg in ['0.0_0.0_0.0', '0.0_0.0_1.0', '1.0_0.0_0.0', '1.0_0.0_1.0', '1.0_1.0_0.0', '1.0_1.0_1.0']]\n",
    "\n",
    "        fair_data = fair_data.iloc[ids, :]\n",
    "        all_data.append(fair_data)\n",
    "    fair_all = pd.concat(all_data)\n",
    "    fair_df = dataset.perform_encoding('cont_ord_cat', fair_all, fair_all)\n",
    "    pca_features_all = tsne(fair_df, dataset)\n",
    "    print('done')\n",
    "    for e, path in enumerate([f'{results_path}/fair_rbu_{dataset_name}_decision_tree/2024-07-14/', f'{results_path}/fair_rbh_{dataset_name}_decision_tree/2024-07-14/', f'{results_path}/hfos_{dataset_name}_decision_tree/2024-06-29/', f'{results_path}/fawos_{dataset_name}_decision_tree/2024-06-29/']):\n",
    "        normal_data = os.path.join(path, f'train_{i}.csv')\n",
    "        fair_data = os.path.join(path, f'fair_{i}.csv')\n",
    "        results = os.path.join(path, f'fairness_{i}.csv')\n",
    "        performance_res = os.path.join(path, f'performance_{i}.csv')\n",
    "        test_data = os.path.join(path, f'test_{i}.csv')\n",
    "        preds_train = os.path.join(path, f'train_preds_{i}.npy')\n",
    "        preds_fair = os.path.join(path, f'fair_preds_{i}.npy')\n",
    "        \n",
    "        normal_data = pd.read_csv(normal_data)\n",
    "        normal_data = normal_data.iloc[: , 1:]\n",
    "        fair_data = pd.read_csv(fair_data)\n",
    "        fair_data = fair_data.iloc[: , 1:]\n",
    "        features, sensitive, target = get_features_groups_target(fair_data, dataset)\n",
    "        target = target[dataset.target]\n",
    "        sensitive_groups = sensitive.astype('str').agg('_'.join, axis=1)\n",
    "        group_class = [f'{s}_{t}' for t, s in zip(target.to_list(), sensitive_groups.to_list())]\n",
    "        ids = [i for i, gg in enumerate(group_class) if gg in ['0.0_0.0_0.0', '0.0_0.0_1.0', '1.0_0.0_0.0', '1.0_0.0_1.0', '1.0_1.0_0.0', '1.0_1.0_1.0']]\n",
    "\n",
    "        fair_data = fair_data.iloc[ids, :]\n",
    "        results = pd.read_csv(results)\n",
    "        performance = pd.read_csv(performance_res)\n",
    "        \n",
    "        if e == 0:\n",
    "            fig = explain_data(normal_data, all_data, dataset, fig, axs, e, '-', pca_features_all)\n",
    "        \n",
    "        fig = explain_data(fair_data, all_data, dataset, fig, axs, e + 1, algorithms[e], pca_features_all)\n",
    "    axs[1,2].remove()\n",
    "    # for ax in axs:\n",
    "    #     ax.get_legend().set_visible(False)\n",
    "    plt.savefig(f'{save_path}/{dataset_name}_{multi_bin}.pdf')\n",
    "    plt.show()\n",
    "    print('========================================================================')"
   ],
   "id": "5ba1a6b97239c38d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2bfbf5988e3de646",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
